{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2 interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/EffiSciencesResearch/ML4G/blob/main/days/w1d6/logit_lens.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logit lens\n",
    "\n",
    "Read : https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens\n",
    "\n",
    "Then try to reimplement it in a minimal way.\n",
    "\n",
    "\n",
    "Resources:\n",
    "- Read about hooks here https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html#forward-and-backward-function-hooks\n",
    "- pip install transformer-utils and use the function _plot_logit_lens https://github.dev/nostalgebraist/transformer-utils/tree/main/src/transformer_utils/logit_lens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers transformer_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "# Minimal example\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model.eval()\n",
    "\n",
    "def text_to_input_ids(text):\n",
    "    toks = tokenizer.encode(text)\n",
    "    return torch.as_tensor(toks).view(1, -1)\n",
    "\n",
    "\n",
    "input_ids = text_to_input_ids(\"Happy birthday to You, happy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers_gpt = len(model.base_model.h)\n",
    "outputs = [None] * n_layers_gpt\n",
    "handles = [None] * n_layers_gpt\n",
    "\n",
    "input_ids = text_to_input_ids(\"Happy birthday to You, happy\")\n",
    "\n",
    "def make_memorize_output_layer(layer):\n",
    "    def memorize_output_layer(self, input, output):\n",
    "        global outputs\n",
    "        outputs[layer] = output[0].detach()\n",
    "    return memorize_output_layer\n",
    "\n",
    "for i, gpt_block in enumerate(model.base_model.h):\n",
    "    handles[i] = gpt_block.register_forward_hook(make_memorize_output_layer(i))\n",
    "model(input_ids);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode\n",
    "ln_f = model.base_model.ln_f\n",
    "\n",
    "layer_preds = []\n",
    "layer_logits = []\n",
    "layer_probs = []\n",
    "\n",
    "for layer_i, output in enumerate(outputs):\n",
    "    normalized_output = ln_f(output)\n",
    "\n",
    "    word_embeddings = model.base_model.wte.weight.detach()\n",
    "    word_distribution = torch.einsum(\"bte,we->btw\", [normalized_output, word_embeddings])\n",
    "    best_word = torch.argmax(word_distribution, dim=2)\n",
    "    output_text = tokenizer.decode(best_word[0])\n",
    "    print(output_text)\n",
    "    \n",
    "    layer_preds.append(best_word) \n",
    "    layer_logits.append(word_distribution) \n",
    "    layer_probs.append(torch.softmax(word_distribution, dim=2)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def to_tensor(list_tensor : List[torch.Tensor]) -> torch.Tensor:\n",
    "    list_tensor = [t.detach() for t in list_tensor]\n",
    "    return torch.concat(list_tensor, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_utils.logit_lens.plotting import _plot_logit_lens\n",
    "_plot_logit_lens(\n",
    "    to_tensor(layer_logits),\n",
    "    to_tensor(layer_preds),\n",
    "    to_tensor(layer_probs),\n",
    "    tokenizer,\n",
    "    input_ids=input_ids,\n",
    "    start_ix=0,\n",
    "    layer_names=[i for i in range(n_layers_gpt)],\n",
    "    probs=False,\n",
    "    ranks=False,\n",
    "    kl=False,\n",
    "    top_down=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2fd0984fba0281b5ef284af29ca774b719b1029bd52fddcabf8232d2cbd23e9e"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('automl37_install')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
