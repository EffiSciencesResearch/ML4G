{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Uhb1Sy1m5SZ"
   },
   "source": [
    "# Weight visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cU7kmNxIm9Ud",
    "outputId": "dc4481b0-d451-4374-9551-2af19c29437e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /home/ubuntu/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f0e4349ebbf4b838b35d2b492749f3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/83.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 15.78 GiB total capacity; 73.15 MiB already allocated; 3.75 MiB free; 88.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15792/4049583798.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.asdf/installs/python/3.9.6/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    897\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m~/.asdf/installs/python/3.9.6/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.asdf/installs/python/3.9.6/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.asdf/installs/python/3.9.6/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.asdf/installs/python/3.9.6/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.asdf/installs/python/3.9.6/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    895\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    896\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 897\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 15.78 GiB total capacity; 73.15 MiB already allocated; 3.75 MiB free; 88.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "model = models.resnet34(pretrained=True)\n",
    "_ = model.eval()\n",
    "device = 'cuda' if torch.cuda.is_available else 'cpu'\n",
    "model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "f0GRDhWrnBoX",
    "outputId": "b4970d9b-37f1-4342-a889-2b7951e245e7"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(8, 8, figsize=(8, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "  weights = model.conv1.weight[i].permute(1, 2, 0).detach().clone().cpu().numpy()\n",
    "  weights = (weights - weights.min()) / (weights.max() - weights.min())\n",
    "  ax.imshow(weights)\n",
    "  ax.set_axis_off()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gmx112HpnufW"
   },
   "source": [
    "# Feature visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "kznBM03eqBaj",
    "outputId": "e16a89f0-f3f0-4d1c-f9ed-8e70cb739bee"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "\n",
    "def load_image(url):\n",
    "  response = requests.get(url)\n",
    "  return Image.open(BytesIO(response.content))\n",
    "\n",
    "urls = [\n",
    "  \"https://www.oregonzoo.org/sites/default/files/styles/article-full/public/animals/H_chimpanzee%20Jackson.jpg\",\n",
    "  \"https://anipassion.com/ow_userfiles/plugins/animal/breed_image_56efffab3e169.jpg\",\n",
    "  # \"https://upload.wikimedia.org/wikipedia/commons/f/f2/Platypus.jpg\",\n",
    "  \"https://static5.depositphotos.com/1017950/406/i/600/depositphotos_4061551-stock-photo-hourglass.jpg\",\n",
    "  \"https://img.nealis.fr/ptv/img/p/g/1465/1464424.jpg\",\n",
    "  \"http://www.tudobembresil.com/wp-content/uploads/2015/11/nouvelancopacabana.jpg\",\n",
    "  \"https://ychef.files.bbci.co.uk/976x549/p0639ffn.jpg\",\n",
    "  \"https://www.thoughtco.com/thmb/Dk3bE4x1qKqrF6LBf2qzZM__LXE=/1333x1000/smart/filters:no_upscale()/iguana2-b554e81fc1834989a715b69d1eb18695.jpg\",\n",
    "  \"https://i.redd.it/mbc00vg3kdr61.jpg\",\n",
    "  \"https://static.wikia.nocookie.net/disneyfanon/images/a/af/Goofy_pulling_his_ears.jpg\",\n",
    "]\n",
    "\n",
    "def deprocess_image(z):\n",
    "    # Normalize array: center on 0.\n",
    "    z -= z.mean()\n",
    "    z /= z.std() + 1e-5\n",
    "    z = (255 * np.clip(z+0.5, 0, 1)).astype(\"uint8\")\n",
    "    return z\n",
    "\n",
    "def normalize_01(x):\n",
    "  EPS = 1e-5\n",
    "  return (x - x.min() + EPS) / (x.max() - x.min() + EPS)\n",
    "\n",
    "# urls = urls[:2]\n",
    "\n",
    "fig, axes = plt.subplots(len(urls), 2, figsize=(9, 3 * len(urls)))\n",
    "\n",
    "_ = [ax.set_axis_off() for ax in axes.flatten()]\n",
    "\n",
    "for i, url in enumerate(urls):\n",
    "  img = load_image(url)\n",
    "  inputs = transforms.ToTensor()(img).unsqueeze_(0)\n",
    "  inputs.requires_grad_()\n",
    "  logits = model(inputs.to(device))\n",
    "  top_class = logits.argmax(-1)[0]\n",
    "  logits[0, top_class].backward()\n",
    "\n",
    "  grad = inputs.grad\n",
    "  out = grad.clone().abs()[0].permute(1, 2, 0)\n",
    "  out = normalize_01(out)\n",
    "  axes[i, 0].imshow(inputs[0].permute(1, 2, 0).detach().numpy())\n",
    "  axes[i, 1].imshow(out, cmap='hot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dDc5_nhrUR6X",
    "outputId": "7b8ed5d9-febf-42a6-9168-797a22b097c5"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 624
    },
    "id": "FCPu3CUq1RPn",
    "outputId": "e54c3e16-1ca5-4582-b5fa-e894d184d243"
   },
   "outputs": [],
   "source": [
    "urls = urls[:3]\n",
    "\n",
    "fig, axes = plt.subplots(len(urls), 2, figsize=(9, 3 * len(urls)))\n",
    "\n",
    "_ = [ax.set_axis_off() for ax in axes.flatten()]\n",
    "\n",
    "random_transform = transforms.Compose([\n",
    "  # transforms.RandAugment(),\n",
    "  transforms.RandomRotation(degrees=15),\n",
    "  transforms.ColorJitter(\n",
    "      brightness=2.0,\n",
    "      contrast=0.5,\n",
    "      saturation=1.0,\n",
    "      hue=0.5,\n",
    "  ),\n",
    "])\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "for i, url in enumerate(urls):\n",
    "  img = load_image(url)\n",
    "\n",
    "  x = transforms.ToTensor()(img)\n",
    "  inputs = torch.empty(batch_size, *x.shape)\n",
    "  for j in range(batch_size):\n",
    "    inputs[j] = random_transform(x)\n",
    "\n",
    "  inputs.requires_grad_()\n",
    "  logits = model(inputs.to(device))\n",
    "  top_class = logits.argmax(-1)[0]\n",
    "  logits[0, top_class].backward()\n",
    "\n",
    "  grad = inputs.grad.mean(0)\n",
    "  out = grad.clone().abs().permute(1, 2, 0)\n",
    "  out = normalize_01(out)\n",
    "  x_np = x.permute(1, 2, 0).detach().numpy()\n",
    "  print(x_np.shape)\n",
    "  print(out.shape)\n",
    "  axes[i, 0].imshow(x_np)\n",
    "  axes[i, 1].imshow(out, cmap='Reds')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GBWEFplgn3px",
    "outputId": "4232efc2-17b4-4623-bc0c-05aa0e38b28b"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "resnet34 = models.resnet34(pretrained=True)\n",
    "resnet34.eval()\n",
    "\n",
    "layer = resnet34.layer3\n",
    "# layer = resnet34.conv1\n",
    "layer = resnet34.layer3[-1]\n",
    "layer = resnet34\n",
    "\n",
    "vals = {}\n",
    "def hook(model, inputs, outputs):\n",
    "  # print(inputs[0].shape)\n",
    "  # print(outputs.shape)\n",
    "  vals['activations'] = outputs\n",
    "\n",
    "layer.register_forward_hook(hook)\n",
    "layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8iMEmjBQix9v",
    "outputId": "05d5eaa9-e029-4328-c137-28ed82a85319"
   },
   "outputs": [],
   "source": [
    "x = torch.randn((1, 3, 64, 64))\n",
    "_ = resnet34(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "uUVR3VDYjus0",
    "outputId": "431c70f3-ec74-4e26-de29-70e987eeb71e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def deprocess_image(z):\n",
    "    # Normalize array: center on 0., ensure variance is 0.15\n",
    "    z -= z.mean()\n",
    "    z /= z.std() + 1e-5\n",
    "    z *= 0.5\n",
    "    z = 255 * np.clip(z+0.5, 0, 1)\n",
    "    return z.astype(\"uint8\")\n",
    "\n",
    "x = torch.randn((1, 3, 255, 255), requires_grad=True)\n",
    "x = torch.nn.Parameter((torch.rand((1, 3, 255, 255)) - 0.5))\n",
    "print(x.shape)\n",
    "print(x.requires_grad)\n",
    "\n",
    "def display_tensor(t):\n",
    "  img = deprocess_image(t[0].permute(1, 2, 0).clone().detach().numpy())\n",
    "  display(Image.fromarray(img))\n",
    "  print('')\n",
    "\n",
    "lr = 1e1\n",
    "n_iter = 200\n",
    "for i in range(n_iter):\n",
    "  if i % (n_iter // 5) == 0:\n",
    "    display_tensor(x)\n",
    "\n",
    "  _ = resnet34(x)\n",
    "  actv = vals['activations']\n",
    "  out = actv[0, 207].mean()\n",
    "  out.backward()\n",
    "  grad = x.grad / torch.norm(x.grad)\n",
    "  x.data += lr * grad\n",
    "  x.grad.zero_()\n",
    "\n",
    "display_tensor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 540
    },
    "id": "2LRlzw7IB6cz",
    "outputId": "42a0ded6-f453-4c26-ef20-854fe8e8b5d7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import fft\n",
    "\n",
    "def deprocess_image(z):\n",
    "    # Normalize array: center on 0.\n",
    "    z -= z.mean()\n",
    "    z /= z.std() + 1e-5\n",
    "    z = (255 * np.clip(z+0.5, 0, 1)).astype(\"uint8\")\n",
    "    return z\n",
    "\n",
    "# x = torch.randn((1, 3, 255, 255), requires_grad=True)\n",
    "h, w = 64, 64\n",
    "p0  = (torch.rand((1, 3, h, w)) - 0.5) * 0.25\n",
    "x_f = torch.nn.Parameter(p0)\n",
    "print(x_f.shape)\n",
    "print(x_f.dtype)\n",
    "x = fft.irfft2(x_f, s=(h, w))\n",
    "\n",
    "def display_tensor(t):\n",
    "  img = deprocess_image(t[0].permute(1, 2, 0).clone().detach().numpy())\n",
    "  display(Image.fromarray(img))\n",
    "  print('')\n",
    "\n",
    "lr = 1e3\n",
    "n_iter = 100\n",
    "for i in range(n_iter):\n",
    "  if i % (n_iter // 5) == 0:\n",
    "    display_tensor(x)\n",
    "\n",
    "  _ = resnet34(x)\n",
    "  actv = vals['activations']\n",
    "  out = actv[0, 0].mean()\n",
    "  out.backward()\n",
    "  # grad = x_f.grad / torch.norm(x_f.grad)\n",
    "  x_f.data += lr * grad\n",
    "  x_f.grad.zero_()\n",
    "\n",
    "display_tensor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "q3KmxGao1d0H",
    "outputId": "b59db105-debc-4a5e-a60c-a5befae23e78"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import fft\n",
    "\n",
    "\n",
    "# From https://github.com/tensorflow/lucid/blob/master/lucid/optvis/param/spatial.py\n",
    "def rfft2d_freqs(h, w):\n",
    "    \"\"\"Computes 2D spectrum frequencies.\"\"\"\n",
    "    fy = np.fft.fftfreq(h)[:, None]\n",
    "    # when we have an odd input dimension we need to keep one additional\n",
    "    # frequency and later cut off 1 pixel\n",
    "    if w % 2 == 1:\n",
    "        fx = np.fft.fftfreq(w)[: w // 2 + 2]\n",
    "    else:\n",
    "        fx = np.fft.fftfreq(w)[: w // 2 + 1]\n",
    "    return np.sqrt(fx * fx + fy * fy)\n",
    "\n",
    "\n",
    "def fft_image(shape, sd=None, decay_power=1):\n",
    "    batch, channels, h, w = shape\n",
    "    freqs = rfft2d_freqs(h, w)\n",
    "    init_val_size = (batch, channels) + freqs.shape + (2,) # 2 for imaginary and real components\n",
    "    sd = sd or 0.01\n",
    "\n",
    "    spectrum_real_imag_t = (torch.randn(*init_val_size) * sd).to(device).requires_grad_(True)\n",
    "\n",
    "    scale = 1.0 / np.maximum(freqs, 1.0 / max(w, h)) ** decay_power\n",
    "    scale = torch.tensor(scale).float()[None, None, ..., None].to(device)\n",
    "\n",
    "    def inner():\n",
    "        scaled_spectrum_t = scale * spectrum_real_imag_t\n",
    "        if TORCH_VERSION >= \"1.7.0\":\n",
    "            import torch.fft\n",
    "            if type(scaled_spectrum_t) is not torch.complex64:\n",
    "                scaled_spectrum_t = torch.view_as_complex(scaled_spectrum_t)\n",
    "            image = torch.fft.irfftn(scaled_spectrum_t, s=(h, w), norm='ortho')\n",
    "        else:\n",
    "            import torch\n",
    "            image = torch.irfft(scaled_spectrum_t, 2, normalized=True, signal_sizes=(h, w))\n",
    "        image = image[:batch, :channels, :h, :w]\n",
    "        magic = 4.0 # Magic constant from Lucid library; increasing this seems to reduce saturation\n",
    "        image = image / magic\n",
    "        return image\n",
    "    return [spectrum_real_imag_t], inner\n",
    "\n",
    "\n",
    "def deprocess_image(z):\n",
    "    # Normalize array: center on 0.\n",
    "    z -= z.mean()\n",
    "    z /= z.std() + 1e-5\n",
    "    z = (255 * np.clip(z+0.5, 0, 1)).astype(\"uint8\")\n",
    "    return z\n",
    "\n",
    "# x = torch.randn((1, 3, 255, 255), requires_grad=True)\n",
    "# h, w = 64, 64\n",
    "h, w = 224, 224\n",
    "p0  = (torch.rand((1, 3, h, w)) - 0.5) * 0.25\n",
    "x_f = torch.nn.Parameter(p0)\n",
    "print(x_f.shape)\n",
    "print(x_f.dtype)\n",
    "\n",
    "# kshape = (1, 3, h, w)\n",
    "# params, x = fft_image(shape)\n",
    "\n",
    "def display_tensor(t):\n",
    "  img = deprocess_image(t[0].permute(1, 2, 0).clone().detach().numpy())\n",
    "  display(Image.fromarray(img))\n",
    "  print('')\n",
    "\n",
    "# params = x_f.parameters()\n",
    "lr = 1e1\n",
    "# opt = optim.Adam(params, lr)\n",
    "\n",
    "n_iter = 50\n",
    "n_show = 10\n",
    "for i in range(n_iter):\n",
    "  x = fft.irfft2(x_f, s=(h, w))\n",
    "  if i % (n_iter // n_show) == 0:\n",
    "    display_tensor(x)\n",
    "\n",
    "  _ = resnet34(x)\n",
    "  actv = vals['activations']\n",
    "  out = actv[0, 207].mean()\n",
    "  out.backward()\n",
    "  grad = x_f.grad / torch.norm(x_f.grad)\n",
    "  x_f.data += lr * grad\n",
    "  x_f.grad.zero_()\n",
    "\n",
    "x = fft.irfft2(x_f, s=(h, w))\n",
    "display_tensor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "cBrJuMhlPP-8",
    "outputId": "b59db105-debc-4a5e-a60c-a5befae23e78"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import fft\n",
    "\n",
    "\n",
    "# From https://github.com/tensorflow/lucid/blob/master/lucid/optvis/param/spatial.py\n",
    "def rfft2d_freqs(h, w):\n",
    "    \"\"\"Computes 2D spectrum frequencies.\"\"\"\n",
    "    fy = np.fft.fftfreq(h)[:, None]\n",
    "    # when we have an odd input dimension we need to keep one additional\n",
    "    # frequency and later cut off 1 pixel\n",
    "    if w % 2 == 1:\n",
    "        fx = np.fft.fftfreq(w)[: w // 2 + 2]\n",
    "    else:\n",
    "        fx = np.fft.fftfreq(w)[: w // 2 + 1]\n",
    "    return np.sqrt(fx * fx + fy * fy)\n",
    "\n",
    "\n",
    "def fft_image(shape, sd=None, decay_power=1):\n",
    "    batch, channels, h, w = shape\n",
    "    freqs = rfft2d_freqs(h, w)\n",
    "    init_val_size = (batch, channels) + freqs.shape + (2,) # 2 for imaginary and real components\n",
    "    sd = sd or 0.01\n",
    "\n",
    "    spectrum_real_imag_t = (torch.randn(*init_val_size) * sd).to(device).requires_grad_(True)\n",
    "\n",
    "    scale = 1.0 / np.maximum(freqs, 1.0 / max(w, h)) ** decay_power\n",
    "    scale = torch.tensor(scale).float()[None, None, ..., None].to(device)\n",
    "\n",
    "    def inner():\n",
    "        scaled_spectrum_t = scale * spectrum_real_imag_t\n",
    "        if TORCH_VERSION >= \"1.7.0\":\n",
    "            import torch.fft\n",
    "            if type(scaled_spectrum_t) is not torch.complex64:\n",
    "                scaled_spectrum_t = torch.view_as_complex(scaled_spectrum_t)\n",
    "            image = torch.fft.irfftn(scaled_spectrum_t, s=(h, w), norm='ortho')\n",
    "        else:\n",
    "            import torch\n",
    "            image = torch.irfft(scaled_spectrum_t, 2, normalized=True, signal_sizes=(h, w))\n",
    "        image = image[:batch, :channels, :h, :w]\n",
    "        magic = 4.0 # Magic constant from Lucid library; increasing this seems to reduce saturation\n",
    "        image = image / magic\n",
    "        return image\n",
    "    return [spectrum_real_imag_t], inner\n",
    "\n",
    "\n",
    "def deprocess_image(z):\n",
    "    # Normalize array: center on 0.\n",
    "    z -= z.mean()\n",
    "    z /= z.std() + 1e-5\n",
    "    z = (255 * np.clip(z+0.5, 0, 1)).astype(\"uint8\")\n",
    "    return z\n",
    "\n",
    "# x = torch.randn((1, 3, 255, 255), requires_grad=True)\n",
    "# h, w = 64, 64\n",
    "h, w = 224, 224\n",
    "p0  = (torch.rand((1, 3, h, w)) - 0.5) * 0.25\n",
    "param, image_f  = fft_image((1, 3, h, w))\n",
    "param = param[0]\n",
    "\n",
    "# kshape = (1, 3, h, w)\n",
    "# params, x = fft_image(shape)\n",
    "\n",
    "def display_tensor(t):\n",
    "  img = deprocess_image(t[0].permute(1, 2, 0).clone().detach().numpy())\n",
    "  display(Image.fromarray(img))\n",
    "  print('')\n",
    "\n",
    "# params = x_f.parameters()\n",
    "lr = 1e1\n",
    "opt = optim.Adam(params, lr)\n",
    "\n",
    "n_iter = 50\n",
    "n_show = 10\n",
    "for i in range(n_iter):\n",
    "  x = fft.irfft2(x_f, s=(h, w))\n",
    "  if i % (n_iter // n_show) == 0:\n",
    "    display_tensor(x)\n",
    "\n",
    "  _ = resnet34(x)\n",
    "  actv = vals['activations']\n",
    "  out = actv[0, 207].mean()\n",
    "  out.backward()\n",
    "  \n",
    "  grad = x_f.grad / torch.norm(x_f.grad)\n",
    "  x_f.data += lr * grad\n",
    "  x_f.grad.zero_()\n",
    "\n",
    "x = fft.irfft2(x_f, s=(h, w))\n",
    "display_tensor(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKe3ADlUmWIC"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pxfmobp5mX8v"
   },
   "source": [
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "BYO-Interpretability-v0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
